{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZWo_wE_U0i4"
   },
   "source": [
    "This notebook explores the use of SentenceBERT to generate representations of sequences (sentences, documents) and clustering those representations using K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsLXe22vZRyY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from sentence-transformers) (4.37.2)\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.2.0-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: nltk in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from sentence-transformers) (3.8.1)\n",
      "Collecting sentencepiece (from sentence-transformers)\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.15.1 in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: Pillow in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from sentence-transformers) (10.0.1)\n",
      "Requirement already satisfied: filelock in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.12.2)\n",
      "Requirement already satisfied: requests in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Collecting sympy (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: click in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/envs/intronlp/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2023.11.17)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m996.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.0-cp311-none-macosx_10_9_x86_64.whl (150.6 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m134.3/150.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0mm"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEvo_V2kZcf5"
   },
   "outputs": [],
   "source": [
    "# Get movies summaries and book titles to cluster\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp23/main/data/plot_summaries.txt\n",
    "!wget https://raw.githubusercontent.com/dbamman/anlp23/main/data/loc/dev.tsv -O book_titles.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPnLGCaGSOGN"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from math import sqrt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j04I_-NfZZEM"
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            idd=cols[0]\n",
    "            summary=cols[1]\n",
    "            data.append((idd, summary))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VviGH3nZiiH"
   },
   "outputs": [],
   "source": [
    "movies=read_data(\"plot_summaries.txt\")\n",
    "book_titles=read_data(\"book_titles.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmChmfWtZmYc"
   },
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HacVXkwQaU9V"
   },
   "outputs": [],
   "source": [
    "embedding=sentence_model.encode(\"this is a sentence\")\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjWG2l73O5MB"
   },
   "outputs": [],
   "source": [
    "def cosine(one, two):\n",
    "  return np.dot(one,two)/(sqrt(np.dot(one,one)) * sqrt(np.dot(two,two)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wi9-2aAay34"
   },
   "outputs": [],
   "source": [
    "def run_all(data, model, num_clusters=10):\n",
    "\n",
    "    X=[]\n",
    "\n",
    "    # Get sentence embeddings for each doc\n",
    "    for idx, doc in data:\n",
    "      embedding=model.encode(doc)\n",
    "      X.append(embedding)\n",
    "\n",
    "    X=np.array(X)\n",
    "\n",
    "    # Run K-means\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
    "\n",
    "    # For each cluster, print out the n documents closest to the cluster center\n",
    "    clusters={}\n",
    "    for idx, label in enumerate(kmeans.labels_):\n",
    "      if label not in clusters:\n",
    "        clusters[label]=[]\n",
    "      clusters[label].append((idx, data[idx][1]))\n",
    "\n",
    "    for label in clusters:\n",
    "      sims={}\n",
    "      cluster_center=kmeans.cluster_centers_[label]\n",
    "      for idx, doc in clusters[label]:\n",
    "        sim=cosine(cluster_center, X[idx])\n",
    "        sims[idx]=sim\n",
    "      for k, v in sorted(sims.items(), key=lambda item: item[1], reverse=True)[:5]:\n",
    "        print(k,\"%.3f\" % v, data[k][1])\n",
    "\n",
    "      print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_hOcnwzVgV9"
   },
   "source": [
    "# Book titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrddyPw1bHdG"
   },
   "outputs": [],
   "source": [
    "run_all(book_titles[:1000], sentence_model, num_clusters=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxDLind6VSTy"
   },
   "source": [
    "# Movie summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngWb-mi8bLfy"
   },
   "outputs": [],
   "source": [
    "run_all(movies[:100], sentence_model, num_clusters=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GgrP1OqVvU4"
   },
   "source": [
    "A1: Play around with this method and vary the number of movies clustered, along with the number of clusters.  How would you rate the coherence and interepretability of these clusters? Try to label some of the clusters and discuss with your neighbors about the overall coherence.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dzUxKcPWHyb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
