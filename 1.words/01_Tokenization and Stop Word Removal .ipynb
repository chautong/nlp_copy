{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65a27c67",
   "metadata": {},
   "source": [
    "## Introduction to Natural Language Processing\n",
    "\n",
    "In recent years, the growth and advancements made in natural language processing have highlighted how impactful this group of algorithms can be to everyday life. Natural language processing (NLP)  can be defined as a subset of machine learning that deals with the analysis and understanding of human language. It enables algorithms to better understand, interpret, and generate natural language. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09f3c6",
   "metadata": {},
   "source": [
    "## NLP Preprocessing Tasks & Tokenization\n",
    "\n",
    "Before diving deep into the nuances of training NLP models, it’s important to first discuss the necessity of preprocessing natural language data. Machines are great at understanding numbers, but not so great at understanding words. Because of this, every implementation of a natural language processing model involves the conversion of text to numbers, however different models differ in the techniques used for this conversion.  While the techniques can slightly differ model to model, it’s important to understand the fundamentals\n",
    "\n",
    "\n",
    "One of the first concepts in NLP data cleaning is tokenization. Tokenization is the process of separating words, phrases and symbols into individual elements. Each small segment is referred to as a token, these tokens can be recombined to create different versions of text used downstream for embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fe08f8",
   "metadata": {},
   "source": [
    "## Using NLTK \n",
    "\n",
    "In order to get started, this notebook leverages the Natural Language Toolkit (NLTK) library. NLTK is a leading platform for building Python programs to work with human language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd59406d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Applications/anaconda3/lib/python3.8/site-packages (3.6.1)\n",
      "Requirement already satisfied: joblib in /Applications/anaconda3/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /Applications/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/lib/python3.8/site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: regex in /Applications/anaconda3/lib/python3.8/site-packages (from nltk) (2021.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "213c63d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/chautong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chautong/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "import nltk\n",
    "nltk.download('punkt') # required dependency\n",
    "nltk.download('stopwords') # required dependency "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15705ce2",
   "metadata": {},
   "source": [
    "We'll start by tokenizing a simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "084e1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = f\"Hey! I'm new in town. Can you please point me in the direction of the groccery store\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bebb7ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Hey! I'm new in town. Can you please point me in the direction of the \"\n",
      " 'groccery store')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41150a15",
   "metadata": {},
   "source": [
    "There are endless ways to tokenize the sample sentences, the first will be to tokenize by sentence. The `sent_tokenize` module will break the corpus up based on end of sentence punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb1ad5",
   "metadata": {},
   "source": [
    "![Tokenization](docs/images/tokenization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd945987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey!',\n",
       " \"I'm new in town.\",\n",
       " 'Can you please point me in the direction of the groccery store']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "## Toeknize the entire sentnece \n",
    "\n",
    "(sent_tokenize(corpus, language=\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89dd655",
   "metadata": {},
   "source": [
    "Another way to tokenize is by words. The `word_tokenize` modules will define tokens by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a411304e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey',\n",
       " '!',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'new',\n",
       " 'in',\n",
       " 'town',\n",
       " '.',\n",
       " 'Can',\n",
       " 'you',\n",
       " 'please',\n",
       " 'point',\n",
       " 'me',\n",
       " 'in',\n",
       " 'the',\n",
       " 'direction',\n",
       " 'of',\n",
       " 'the',\n",
       " 'groccery',\n",
       " 'store']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "# Tokenize by word\n",
    "\n",
    "(word_tokenize(corpus, language=\"english\", preserve_line=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "839ecf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hey', '!'],\n",
       " ['I', \"'m\", 'new', 'in', 'town', '.'],\n",
       " ['Can',\n",
       "  'you',\n",
       "  'please',\n",
       "  'point',\n",
       "  'me',\n",
       "  'in',\n",
       "  'the',\n",
       "  'direction',\n",
       "  'of',\n",
       "  'the',\n",
       "  'groccery',\n",
       "  'store']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize every word in both sentences separately \n",
    "\n",
    "[word_tokenize(character, \n",
    "               language=\"english\", \n",
    "               preserve_line=False) for character in sent_tokenize(corpus, language=\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84fa7b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey',\n",
       " '!',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'new',\n",
       " 'in',\n",
       " 'town',\n",
       " '.',\n",
       " 'Can',\n",
       " 'you',\n",
       " 'please',\n",
       " 'point',\n",
       " 'me',\n",
       " 'in',\n",
       " 'the',\n",
       " 'direction',\n",
       " 'of',\n",
       " 'the',\n",
       " 'groccery',\n",
       " 'store']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "# Tokenize by word and punctuation \n",
    "\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c7bf07",
   "metadata": {},
   "source": [
    "## Stopwords \n",
    " \n",
    " \n",
    "Once tokenized, the next step is to identify and remove stop words. Stop words refer to common words in a language that aren’t considered to be helpful in understanding text. In the English language, these are words like “the”, “a”, “and”, “in”, etc. Because these words carry such little information, they are often removed to ensure that the model only uses words that carry impact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98714aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "## NLTK has several built in stop words \n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28dc008b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', '!', \"'m\", 'new', 'town', '.', 'please', 'point', 'direction', 'groccery', 'store']\n"
     ]
    }
   ],
   "source": [
    "## Remvoing stopwords from the sentence\n",
    "\n",
    "english_stop_words = stopwords.words(\"english\")\n",
    "tokens = word_tokenize(corpus.lower())\n",
    "filtered_sentence = [word for word in tokens if not word in english_stop_words]\n",
    "\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adf37f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Say we wantted to add `please` as a stop word, in NLTK the stop word\n",
    "## list can be extended \n",
    "\n",
    "english_stop_words.append(\"please\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18e38c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', '!', \"'m\", 'new', 'town', '.', 'point', 'direction', 'groccery', 'store']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = [word for word in tokens if not word in english_stop_words]\n",
    "\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e48e8a",
   "metadata": {},
   "source": [
    "### Stopwords in Alternative Languages \n",
    "\n",
    "\n",
    "The example above is for English stop words, however NLTK supports several languages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "668359a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d805079",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words(\"italian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfee6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
