{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841157cb",
   "metadata": {},
   "source": [
    "# Neural Network Based Embeddings \n",
    "\n",
    "Because relationships in natural language are complex and nonlinear, deep learning models have quickly emerged as an alternative to counting based techniques to generate embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317db7a0",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Word2Vec is one of those neural networks based methods that generates embeddings from tokenized, processed text. Various Word2Vec implementations leverage different architecture for the networks, but two common ones are the **Continuous Bag of Words (CBOW)** architecture and the **skip-gram** architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56658d06",
   "metadata": {},
   "source": [
    "![NN](docs/images/nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef3c5a0",
   "metadata": {},
   "source": [
    "## CBOW vs. Skip Gram\n",
    "\n",
    "\n",
    " CBOW, a feed forward neural network, selects a target and uses distributed representations of the context surrounding the target to predict the target word. The skip-gram architecture is a bit simpler with one hidden layer and strives to predict the probability of a word being present given various inputs. Conceptually, it reverses the input and output of the CBOW approach. The current word is taken as input to the model and it attempts to predict the context around the input word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce21462",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f99052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a7126",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = f\"Hey! I'm new in town. Can you please point me in the direction of the groccery store\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0992f553",
   "metadata": {},
   "source": [
    "In order to pre-process our data for the `Word2Vec` model, we'll first need to tokenize the corpus by sentence then by word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize by sentence and word \n",
    "data = []\n",
    "\n",
    "for i in sent_tokenize(corpus):\n",
    "    temp = []\n",
    "    \n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "        \n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d2958",
   "metadata": {},
   "source": [
    "After we've tokenized, we can train a `Word2Vec` model using the small corpus above. Training this model will allow for simliarty calcuations downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d287202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec using CBOW\n",
    "cbow = Word2Vec(data, min_count=1, vector_size=100, window=5, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13030284",
   "metadata": {},
   "source": [
    "The `Word2Vec` class has several parameters.\n",
    "- The `min_count` parameter will ignore any words with less than a single frequency. \n",
    "- The `vector_size` parameter limits the dimensionality of the feature vector\n",
    "- The `window` parameter maps the max distance between the current and predicted word within a sentence \n",
    "- The `sg` parameter controls algorithm. Setting `sg=0` means the CBOW is used, while `sg=1` means that the Skip-Gram algorithm is used \n",
    "\n",
    "Read more about the various parameters [here](https://tedboy.github.io/nlps/_modules/gensim/models/word2vec.html#Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfbe959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec using Skip Gram\n",
    "skip_gram = Word2Vec(data, min_count=1, vector_size=100, window=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58578c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarities \n",
    "cbow_similarity = cbow.wv.similarity(\"town\", \"direction\")\n",
    "skip_gram_similarity = skip_gram.wv.similarity(\"town\", \"direction\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Cosine similarity between `town` and `direction` using CBOW Model: {cbow_similarity}\")\n",
    "print(f\"Cosine similarity between `town` and `direction` using Skip Gram Model: {skip_gram_similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9c6a62",
   "metadata": {},
   "source": [
    "Since our corpus is so small, it's likely that these values will be very similar. As the data grows, the `Word2Vec` model is able to pick up on nuances between words. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
